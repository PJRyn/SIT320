{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is super simple importing the libararies needed.\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is what we use to convert a variable to a binary sring.\n",
    "Takes in the key, then uses format to format it to a 16 len binary string.\n",
    "Important to not that it is explicitly a string, not an int.\n",
    "'''\n",
    "def hash_funtion(key):\n",
    "    \n",
    "    return  '{0:016b}'.format(key)\n",
    "\n",
    "'''\n",
    "Here we define a bucket\n",
    "We set the ID for the bucket, this is defined when making the bucket\n",
    "local depth defines the ammount of values for each bucket\n",
    "index is the values stored within\n",
    "empty spaces defines how much room is left in the bucket\n",
    "'''\n",
    "class Bucket:\n",
    "    \n",
    "    def __init__(self,local_depth,index,empty_spaces,id):\n",
    "        \n",
    "        self.id = id\n",
    "        self.local_depth = local_depth\n",
    "        self.index = index\n",
    "        self.empty_spaces = empty_spaces\n",
    "\n",
    "'''\n",
    "This is the central data strcuture that holds the directories that then hold the buckets\n",
    "global_depth how many bits since 2^Global depth\n",
    "    e.g 1 bit [0|1] (size/depth 1) directory size, 2 bits [00|01|10|11] (size/depth 2)\n",
    "directory_records is how many records are being kept\n",
    "'''\n",
    "\n",
    "class Directory:\n",
    "    \n",
    "    def  __init__(self,global_depth,directory_records):\n",
    "        \n",
    "        self.global_depth = global_depth,\n",
    "        self.directory_records = directory_records\n",
    "\n",
    "'''\n",
    "Here is stored each of the \n",
    "Hash prefix defines what hashes it takes, but is listed in decimal form\n",
    "    We can see if in decimal form using the hash_funtion()\n",
    "value is what bucket it is connected to\n",
    "'''\n",
    "\n",
    "class DirectoryRecord:\n",
    "    \n",
    "    def __init__(self,bucket, hash_prefix):\n",
    "        \n",
    "        self.hash_prefix = hash_prefix\n",
    "        self.value = bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory --- (holds) ---> DirectoryRecord(s) ----(holds) ---> Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are just stating some global variables\n",
    "# How many items can be stored in our buckets\n",
    "bucket_capacity = 2\n",
    "#Set the number of initial buckets for ID\n",
    "bucket_number = 3\n",
    "# Set it to have a depth of 2\n",
    "global_depth = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Initialization of buckets\n",
    "    -local depth is how many values in an index we can store (keeping in mind 0 indexing on a list)\n",
    "    -empty_spaces is how many slots are in our bucket (called as bucket capacity here, i guess. Not great naming)\n",
    "    -set index to be a list that is empty, important since stored values must be a list\n",
    "    -ID is a an identifier\n",
    "'''\n",
    "bucket1 = Bucket(local_depth = 1, empty_spaces = bucket_capacity, index = [], id = 1)\n",
    "bucket2 = Bucket(local_depth = 1, empty_spaces = bucket_capacity, index = [], id = 2)\n",
    "'''\n",
    "Here we create a list of the directory_records since they need to be in a list for our directory to work.\n",
    "\n",
    "They have a set hash prefix in decimal as well as an assigned bucket\n",
    "'''\n",
    "directory_records = list()  \n",
    "directory_records.append(DirectoryRecord(hash_prefix = 0, bucket = bucket1))\n",
    "directory_records.append(DirectoryRecord(hash_prefix = 1, bucket = bucket2))\n",
    "'''\n",
    "Finally we add the directory record lists to a directory object as well as setting our globabl depth to be 1\n",
    "since this is a binary depth (2 directory_records)\n",
    "'''\n",
    "directory = Directory(global_depth = 1, directory_records = directory_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insertion Algorithm\n",
    "Here we are using this to insert variables, it is important to know the steps:\n",
    "1. Analyse data - Data elements may exist in many forms e.g int, sting float. So we look at the binary\n",
    "2. Initialise the bucket depth and the global depth of the directories\n",
    "3. Convert into a binary format\n",
    "4. Consider the “Global Depth” number of the least significant bits (LSBs) (bits at the end) of data\n",
    "5. Map the data according to the ID of a directory\n",
    "6. Check the following conditions if a bucket overflows (Exceeds size limit)\n",
    "- Global depth == Bucket depth (local depth) split the bucket into two and increment the global depth and the bucket's depth. Re-hash the elements that were present in the split bucket\n",
    "- Global depth > Bucket depth (local depth): Split the bucket into two and increment the bucket depth only. Re-hash the elements that were present in the split bucket\n",
    "7. Repeat the process for each element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert(index):\n",
    "    \n",
    "    #set the directoy and the bucket number (global since it calls from outside the function)\n",
    "    global directory\n",
    "    global bucket_number\n",
    "    '''\n",
    "    STEP 3\n",
    "    '''\n",
    "    #Take the first variable in the index and hash it\n",
    "    t_id = index[0]\n",
    "    hash_key = hash_funtion(int(t_id))\n",
    "    \n",
    "    '''\n",
    "    STEP 4\n",
    "    '''\n",
    "    #This finds what hash directory matches the inserted value\n",
    "    hash_prefix = int(hash_key[-directory.global_depth[0]:], 2)\n",
    "\n",
    "    '''\n",
    "    STEP 5\n",
    "    '''\n",
    "    #This matches the directory to the bucket based on the hash value\n",
    "    bucket = directory.directory_records[hash_prefix].value\n",
    "    # Add the value to the bucket\n",
    "    bucket.index.append(index)\n",
    "    # Since we put an item inside the bucket we lose a space within it\n",
    "    bucket.empty_spaces = int(bucket.empty_spaces)-1\n",
    "\n",
    "    '''\n",
    "    STEP 6\n",
    "    '''\n",
    "    # This only occurs when there is an overflow\n",
    "    if(bucket.empty_spaces < 0):\n",
    "        \n",
    "        # Create a way to store the bucket index\n",
    "        tempopary_memory = bucket.index   \n",
    "        # Reset the empty spaces \n",
    "        bucket.empty_spaces = bucket_capacity\n",
    "        # Clear its sotred values\n",
    "        bucket.index = []\n",
    "\n",
    "        '''\n",
    "        STEP 6A \n",
    "        occurs if the depth is larger than bucket depth, meaning we only need to split the bucket\n",
    "        '''\n",
    "        if (directory.global_depth[0] > bucket.local_depth):\n",
    "\n",
    "            # NUMBER OF LINKED BUCKETS\n",
    "            number_of_links = 2*(directory.global_depth[0] - bucket.local_depth)\n",
    "            # Increase the depth of the bucket by 1\n",
    "            bucket.local_depth = bucket.local_depth + 1\n",
    "            number_of_modify_links = number_of_links/2 \n",
    "\n",
    "            #Create new bucket\n",
    "            new_bucket = Bucket(local_depth = bucket.local_depth, index=[], empty_spaces = bucket_capacity, id = bucket_number)\n",
    "\n",
    "            #Go through each record \n",
    "            for directory_record in directory.directory_records:\n",
    "                #Check for the bucket\n",
    "                if(directory_record.value == bucket):\n",
    "                    #Each number of links to buckets is then lowered by 1 untill a the new bucket is found\n",
    "                    if(number_of_modify_links != 0):\n",
    "                        number_of_modify_links = number_of_modify_links - 1\n",
    "                    #When it is found link the new bucket and increase the amount of buckets\n",
    "                    else:\n",
    "                        directory_record.value = new_bucket\n",
    "                        bucket_number = bucket_number + 1\n",
    "\n",
    "            # Insert the temprorary bucket back into the list\n",
    "            for i in range(len(tempopary_memory)):\n",
    "                insert(tempopary_memory[i])\n",
    "                \n",
    "            '''\n",
    "            STEP 6B \n",
    "            Occurs when the bucket is the same size as the depth since depth needs to be increased with bucket split\n",
    "            '''\n",
    "        elif (directory.global_depth[0] == bucket.local_depth):\n",
    "            # Create a new length for the directory that is double (since we work in binary)\n",
    "            new_directory_len = 2 * len(directory.directory_records)\n",
    "            # Set a new cleared records list\n",
    "            new_directory_records = []\n",
    "\n",
    "            # Iterate through the length of new directory\n",
    "            for directory_record_number in range(new_directory_len):\n",
    "                \n",
    "                new_directory_records.append(DirectoryRecord(hash_prefix=directory_record_number,bucket=Bucket(local_depth=1,index=[],empty_spaces=bucket_capacity,id=bucket_number)))\n",
    "                # Increase the number of buckets for ID\n",
    "                bucket_number = bucket_number + 1\n",
    "            \n",
    "            # Create the new directory\n",
    "            new_directory = Directory(global_depth=directory.global_depth[0]+1,directory_records=new_directory_records)\n",
    "\n",
    "            # REHASING\n",
    "\n",
    "            #iterate through records\n",
    "            for directory_record in directory.directory_records:\n",
    "                #Adjust the hashes for previous entries \n",
    "                haskey1 = '0'+hash_funtion(directory_record.hash_prefix)\n",
    "                haskey2 = '1'+hash_funtion(directory_record.hash_prefix)\n",
    "                #Crete new hash indexes for them\n",
    "                new_index1 = int(haskey1[-directory.global_depth[0]:],2)\n",
    "                new_index2 = int(haskey2[-directory.global_depth[0]:],2)\n",
    "                #Add the doubled directroeis to the new dorectory\n",
    "                new_directory.directory_records[new_index1].value = directory_record.value\n",
    "                new_directory.directory_records[new_index2].value = directory_record.value\n",
    "            #Set the directy as the new one we had created\n",
    "            directory= new_directory\n",
    "\n",
    "            # We now re-add each value form temporary memory in again with the adjusted hash index. Recursivly\n",
    "            for i in range(len(tempopary_memory)):\n",
    "\n",
    "                insert(tempopary_memory[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A - Simple use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash Prefix:  0000000000000000\n",
      "Bucket ID:  1\n",
      "Local Depth:  1\n",
      "Stored Values:  [[0, 100, 'David'], [4, 102, 'David2']]\n",
      "Hash Prefix:  0000000000000001\n",
      "Bucket ID:  2\n",
      "Local Depth:  1\n",
      "Stored Values:  [[1, 101, 'David2']]\n",
      "Hash Prefix:  0000000000000010\n",
      "Bucket ID:  5\n",
      "Local Depth:  1\n",
      "Stored Values:  [[2, 102, 'David2']]\n",
      "Hash Prefix:  0000000000000011\n",
      "Bucket ID:  6\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n"
     ]
    }
   ],
   "source": [
    "t_id = 0\n",
    "t_amount = 100\n",
    "u_name = 'David'\n",
    "\n",
    "insert([t_id, t_amount, u_name])\n",
    "t_id = 1\n",
    "t_amount = 101\n",
    "u_name = 'David2'\n",
    "insert([t_id, t_amount, u_name])\n",
    "\n",
    "t_id = 2\n",
    "t_amount = 102\n",
    "u_name = 'David2'\n",
    "insert([t_id, t_amount, u_name])\n",
    "\n",
    "t_id = 4\n",
    "t_amount = 102\n",
    "u_name = 'David2'\n",
    "insert([t_id, t_amount, u_name])\n",
    "\n",
    "for i in range(len(directory.directory_records)):\n",
    "    print(\"Hash Prefix: \",hash_funtion(directory.directory_records[i].hash_prefix))\n",
    "    print(\"Bucket ID: \",directory.directory_records[i].value.id)\n",
    "    print(\"Local Depth: \",directory.directory_records[i].value.local_depth)\n",
    "    print(\"Stored Values: \",directory.directory_records[i].value.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash Prefix:  0000000000000000\n",
      "Bucket ID:  1\n",
      "Local Depth:  2\n",
      "Stored Values:  [[0, 100, 'David'], [8, 102, 'David2']]\n",
      "Hash Prefix:  0000000000000001\n",
      "Bucket ID:  2\n",
      "Local Depth:  1\n",
      "Stored Values:  [[1, 101, 'David2']]\n",
      "Hash Prefix:  0000000000000010\n",
      "Bucket ID:  5\n",
      "Local Depth:  1\n",
      "Stored Values:  [[2, 102, 'David2']]\n",
      "Hash Prefix:  0000000000000011\n",
      "Bucket ID:  6\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000000100\n",
      "Bucket ID:  11\n",
      "Local Depth:  1\n",
      "Stored Values:  [[4, 102, 'David2']]\n",
      "Hash Prefix:  0000000000000101\n",
      "Bucket ID:  12\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000000110\n",
      "Bucket ID:  13\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000000111\n",
      "Bucket ID:  14\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n"
     ]
    }
   ],
   "source": [
    "t_id = 8\n",
    "t_amount = 102\n",
    "u_name = 'David2'\n",
    "insert([t_id, t_amount, u_name])\n",
    "\n",
    "for i in range(len(directory.directory_records)):\n",
    "    print(\"Hash Prefix: \",hash_funtion(directory.directory_records[i].hash_prefix))\n",
    "    print(\"Bucket ID: \",directory.directory_records[i].value.id)\n",
    "    print(\"Local Depth: \",directory.directory_records[i].value.local_depth)\n",
    "    print(\"Stored Values: \",directory.directory_records[i].value.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A - Import Random And CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket1 = Bucket(local_depth = 1, empty_spaces = bucket_capacity, index = [], id = 1)\n",
    "bucket2 = Bucket(local_depth = 1, empty_spaces = bucket_capacity, index = [], id = 2)\n",
    "directory_records = list()  \n",
    "directory_records.append(DirectoryRecord(hash_prefix = 0, bucket = bucket1))\n",
    "directory_records.append(DirectoryRecord(hash_prefix = 1, bucket = bucket2))\n",
    "directory = Directory(global_depth = 1, directory_records = directory_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 8, 17, 16, 6, 21, 13, 27, 14, 10, 18, 9, 4]\n"
     ]
    }
   ],
   "source": [
    "#13 random numbers like the task before (part 1)\n",
    "sampleNumbers = random.sample(range(1, 30), 13)\n",
    "print(sampleNumbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"randomNumber.csv\", sampleNumbers, delimiter=\",\", fmt='%s', header='Numbers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['25'], ['8'], ['17'], ['16'], ['6'], ['21'], ['13'], ['27'], ['14'], ['10'], ['18'], ['9'], ['4']]\n"
     ]
    }
   ],
   "source": [
    "file = open(\"randomNumber.csv\", \"r\")\n",
    "importData = list(csv.reader(file, delimiter=\",\"))\n",
    "file.close()\n",
    "importData.pop(0)\n",
    "print(importData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(importData)):\n",
    "    insert(importData[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Depth:  (4,)\n",
      "Count of directory records:  16\n",
      "Hash Prefix:  0000000000000000\n",
      "Bucket ID:  1\n",
      "Local Depth:  1\n",
      "Stored Values:  [['8'], ['16']]\n",
      "Hash Prefix:  0000000000000001\n",
      "Bucket ID:  2\n",
      "Local Depth:  3\n",
      "Stored Values:  [['17']]\n",
      "Hash Prefix:  0000000000000010\n",
      "Bucket ID:  17\n",
      "Local Depth:  2\n",
      "Stored Values:  [['10'], ['18']]\n",
      "Hash Prefix:  0000000000000011\n",
      "Bucket ID:  18\n",
      "Local Depth:  1\n",
      "Stored Values:  [['27']]\n",
      "Hash Prefix:  0000000000000100\n",
      "Bucket ID:  23\n",
      "Local Depth:  1\n",
      "Stored Values:  [['4']]\n",
      "Hash Prefix:  0000000000000101\n",
      "Bucket ID:  24\n",
      "Local Depth:  1\n",
      "Stored Values:  [['21'], ['13']]\n",
      "Hash Prefix:  0000000000000110\n",
      "Bucket ID:  25\n",
      "Local Depth:  1\n",
      "Stored Values:  [['14'], ['6']]\n",
      "Hash Prefix:  0000000000000111\n",
      "Bucket ID:  26\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001000\n",
      "Bucket ID:  35\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001001\n",
      "Bucket ID:  36\n",
      "Local Depth:  1\n",
      "Stored Values:  [['25'], ['9']]\n",
      "Hash Prefix:  0000000000001010\n",
      "Bucket ID:  37\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001011\n",
      "Bucket ID:  38\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001100\n",
      "Bucket ID:  39\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001101\n",
      "Bucket ID:  40\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001110\n",
      "Bucket ID:  41\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001111\n",
      "Bucket ID:  42\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n"
     ]
    }
   ],
   "source": [
    "print(\"Global Depth: \", directory.global_depth)\n",
    "print(\"Count of directory records: \", len(directory.directory_records))\n",
    "for i in range(len(directory.directory_records)):\n",
    "    print(\"Hash Prefix: \",hash_funtion(directory.directory_records[i].hash_prefix))\n",
    "    print(\"Bucket ID: \",directory.directory_records[i].value.id)\n",
    "    print(\"Local Depth: \",directory.directory_records[i].value.local_depth)\n",
    "    print(\"Stored Values: \",directory.directory_records[i].value.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B - Modification - Simple refactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDirectory(index):\n",
    "    print(\"Global Depth: \", index.global_depth)\n",
    "    print(\"Count of directory records: \", len(index.directory_records))\n",
    "    for i in range(len(index.directory_records)):\n",
    "        print(\"Hash Prefix: \",hash_funtion(index.directory_records[i].hash_prefix))\n",
    "        print(\"Bucket ID: \",index.directory_records[i].value.id)\n",
    "        print(\"Local Depth: \",index.directory_records[i].value.local_depth)\n",
    "        print(\"Stored Values: \",index.directory_records[i].value.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Depth:  (4,)\n",
      "Count of directory records:  16\n",
      "Hash Prefix:  0000000000000000\n",
      "Bucket ID:  1\n",
      "Local Depth:  1\n",
      "Stored Values:  [['8'], ['16']]\n",
      "Hash Prefix:  0000000000000001\n",
      "Bucket ID:  2\n",
      "Local Depth:  3\n",
      "Stored Values:  [['17']]\n",
      "Hash Prefix:  0000000000000010\n",
      "Bucket ID:  17\n",
      "Local Depth:  2\n",
      "Stored Values:  [['10'], ['18']]\n",
      "Hash Prefix:  0000000000000011\n",
      "Bucket ID:  18\n",
      "Local Depth:  1\n",
      "Stored Values:  [['27']]\n",
      "Hash Prefix:  0000000000000100\n",
      "Bucket ID:  23\n",
      "Local Depth:  1\n",
      "Stored Values:  [['4']]\n",
      "Hash Prefix:  0000000000000101\n",
      "Bucket ID:  24\n",
      "Local Depth:  1\n",
      "Stored Values:  [['21'], ['13']]\n",
      "Hash Prefix:  0000000000000110\n",
      "Bucket ID:  25\n",
      "Local Depth:  1\n",
      "Stored Values:  [['14'], ['6']]\n",
      "Hash Prefix:  0000000000000111\n",
      "Bucket ID:  26\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001000\n",
      "Bucket ID:  35\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001001\n",
      "Bucket ID:  36\n",
      "Local Depth:  1\n",
      "Stored Values:  [['25'], ['9']]\n",
      "Hash Prefix:  0000000000001010\n",
      "Bucket ID:  37\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001011\n",
      "Bucket ID:  38\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001100\n",
      "Bucket ID:  39\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001101\n",
      "Bucket ID:  40\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001110\n",
      "Bucket ID:  41\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001111\n",
      "Bucket ID:  42\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n"
     ]
    }
   ],
   "source": [
    "printDirectory(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 External Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[94, 78, 55, 1], [72, 80, 6, 93], [59, 86, 32, 19], [60, 49, 48, 47], [31, 85, 57, 23]]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "diskList = []\n",
    "sampleCount = 20\n",
    "pageSize = 4\n",
    "pageCount = int(sampleCount / pageSize)  + 1\n",
    "inputBufferCount = 2\n",
    "\n",
    "samples = random.sample(range(1, 99), sampleCount)\n",
    "\n",
    "for i in range(pageCount):\n",
    "    if not len(samples):\n",
    "        break\n",
    "    samplePage = []\n",
    "    for j in range(pageSize):\n",
    "        if not len(samples):\n",
    "            break\n",
    "        samplePage.append(samples.pop())\n",
    "    diskList.append(samplePage)\n",
    "\n",
    "print(diskList)\n",
    "print(len(diskList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 55, 78, 94], [6, 72, 80, 93], [19, 32, 59, 86], [47, 48, 49, 60], [31, 85, 57, 23]]\n"
     ]
    }
   ],
   "source": [
    "# Pass 1\n",
    "# Diskoutput = []\n",
    "for i in range(0, len(diskList) - 1):\n",
    "    initalSort = np.sort(diskList[i])\n",
    "    diskList[i] = initalSort.tolist()\n",
    "print(diskList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampleIndex = 0\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 0, smallValue = 1\n",
      "major = 1\n",
      "sampleIndex = 1\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 1, smallValue = 55\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 2\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 2, smallValue = 78\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 3\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 3, smallValue = 94\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 4\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 0, smallValue = 19\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 5\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 1, smallValue = 32\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 6\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 2, smallValue = 59\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 7\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 3, smallValue = 86\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 8\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 0, smallValue = 47\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 9\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 1, smallValue = 48\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 10\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 2, smallValue = 49\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 11\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 3, smallValue = 60\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 12\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 0, smallValue = 31\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 13\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 1, smallValue = 85\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 14\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 2, smallValue = 57\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 15\n",
      "major = 0\n",
      "major = 1\n",
      "searching for small values\n",
      "major = 0\n",
      "major = 0 - Range we are going up to = 2\n",
      "smallValue was none - smallMinor = 3, smallValue = 23\n",
      "major = 1\n",
      "found smaller - smallMinor = 0, smallMajor = 6\n",
      "sampleIndex = 16\n",
      "major = 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputBufferIndexes[major] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m pageSize:\n\u001b[1;32m     19\u001b[0m         inputBufferIndexes[major] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 20\u001b[0m         inputBuffers[major] \u001b[38;5;241m=\u001b[39m \u001b[43mdiskList\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputPageIndex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     21\u001b[0m         inputPageIndex \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m smallMajor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "inputBuffers = [None] * inputBufferCount\n",
    "inputBufferIndexes = [pageSize] * inputBufferCount\n",
    "\n",
    "sampleIndex = 0\n",
    "\n",
    "inputPageIndex = 0\n",
    "outputPageIndex = 0\n",
    "outputBuffer = [None] * pageSize\n",
    "outputBufferMinor = 0\n",
    "\n",
    "while sampleIndex <= sampleCount:\n",
    "\n",
    "    print(\"sampleIndex = {}\".format(sampleIndex))\n",
    "\n",
    "    # fill any drained buffers\n",
    "    for major in (range(0, inputBufferCount)):\n",
    "        print(\"major = {}\".format(major))\n",
    "        if inputBufferIndexes[major] >= pageSize:\n",
    "            inputBufferIndexes[major] = 0\n",
    "            inputBuffers[major] = diskList[inputPageIndex]\n",
    "            inputPageIndex += 1\n",
    "\n",
    "    smallMajor = 0\n",
    "    smallMinor = 0\n",
    "    smallValue = None\n",
    "\n",
    "    print(\"searching for small values\")\n",
    "    for major in (range(0, inputBufferCount)): #Related since it may be taking out of range, error occurs when set>\n",
    "        minor = inputBufferIndexes[major]\n",
    "        print(\"major = {}\".format(major))\n",
    "\n",
    "        if smallValue is None:\n",
    "            print(\"major = {} - Range we are going up to = {}\".format(major, len(range(0, inputBufferCount))))\n",
    "            smallMinor = inputBufferIndexes[major]\n",
    "            smallValue = inputBuffers[major][minor] #error Line\n",
    "            print(\"smallValue was none - smallMinor = {}, smallValue = {}\".format(smallMinor, smallValue))\n",
    "    \n",
    "        if inputBuffers[major][minor] < smallValue:\n",
    "            smallMinor = inputBufferIndexes[major]\n",
    "            smallValue = inputBuffers[major][minor]\n",
    "            print(\"found smaller - smallMinor = {}, smallMajor = {}\".format(smallMinor, smallValue))\n",
    "\n",
    "\n",
    "    # bump our bufer index to indicate we used this value\n",
    "    inputBufferIndexes[smallMajor] += 1\n",
    "\n",
    "    # copy our small value into the output buffer and bump\n",
    "    outputBuffer[outputBufferMinor] = smallValue\n",
    "    outputBufferMinor += 1\n",
    "\n",
    "    # if we're off the end of our output buffer, write it out\n",
    "    if outputBufferMinor >= pageSize:\n",
    "        diskList[outputPageIndex] = outputBuffer\n",
    "        outputPageIndex += 1\n",
    "\n",
    "        outputBufferMinor = 0\n",
    "        outputBuffer = [None] * pageSize\n",
    " \n",
    "    sampleIndex += 1 \n",
    "\n",
    "print(inputBuffers)\n",
    "print(inputBufferIndexes)\n",
    "print(pageIndex)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
