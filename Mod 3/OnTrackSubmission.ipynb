{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is super simple importing the libararies needed.\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import math\n",
    "import heapq\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is what we use to convert a variable to a binary sring.\n",
    "Takes in the key, then uses format to format it to a 16 len binary string.\n",
    "Important to not that it is explicitly a string, not an int.\n",
    "'''\n",
    "def hash_funtion(key):\n",
    "    \n",
    "    return  '{0:016b}'.format(key)\n",
    "\n",
    "'''\n",
    "Here we define a bucket\n",
    "We set the ID for the bucket, this is defined when making the bucket\n",
    "local depth defines the ammount of values for each bucket\n",
    "index is the values stored within\n",
    "empty spaces defines how much room is left in the bucket\n",
    "'''\n",
    "class Bucket:\n",
    "    \n",
    "    def __init__(self,local_depth,index,empty_spaces,id):\n",
    "        \n",
    "        self.id = id\n",
    "        self.local_depth = local_depth\n",
    "        self.index = index\n",
    "        self.empty_spaces = empty_spaces\n",
    "\n",
    "'''\n",
    "This is the central data strcuture that holds the directories that then hold the buckets\n",
    "global_depth how many bits since 2^Global depth\n",
    "    e.g 1 bit [0|1] (size/depth 1) directory size, 2 bits [00|01|10|11] (size/depth 2)\n",
    "directory_records is how many records are being kept\n",
    "'''\n",
    "\n",
    "class Directory:\n",
    "    \n",
    "    def  __init__(self,global_depth,directory_records):\n",
    "        \n",
    "        self.global_depth = global_depth,\n",
    "        self.directory_records = directory_records\n",
    "\n",
    "'''\n",
    "Here is stored each of the \n",
    "Hash prefix defines what hashes it takes, but is listed in decimal form\n",
    "    We can see if in decimal form using the hash_funtion()\n",
    "value is what bucket it is connected to\n",
    "'''\n",
    "\n",
    "class DirectoryRecord:\n",
    "    \n",
    "    def __init__(self,bucket, hash_prefix):\n",
    "        \n",
    "        self.hash_prefix = hash_prefix\n",
    "        self.value = bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory --- (holds) ---> DirectoryRecord(s) ----(holds) ---> Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are just stating some global variables\n",
    "# How many items can be stored in our buckets\n",
    "bucket_capacity = 2\n",
    "#Set the number of initial buckets for ID\n",
    "bucket_number = 3\n",
    "# Set it to have a depth of 2\n",
    "global_depth = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Initialization of buckets\n",
    "    -local depth is how many values in an index we can store (keeping in mind 0 indexing on a list)\n",
    "    -empty_spaces is how many slots are in our bucket (called as bucket capacity here, i guess. Not great naming)\n",
    "    -set index to be a list that is empty, important since stored values must be a list\n",
    "    -ID is a an identifier\n",
    "'''\n",
    "bucket1 = Bucket(local_depth = 1, empty_spaces = bucket_capacity, index = [], id = 1)\n",
    "bucket2 = Bucket(local_depth = 1, empty_spaces = bucket_capacity, index = [], id = 2)\n",
    "'''\n",
    "Here we create a list of the directory_records since they need to be in a list for our directory to work.\n",
    "\n",
    "They have a set hash prefix in decimal as well as an assigned bucket\n",
    "'''\n",
    "directory_records = list()  \n",
    "directory_records.append(DirectoryRecord(hash_prefix = 0, bucket = bucket1))\n",
    "directory_records.append(DirectoryRecord(hash_prefix = 1, bucket = bucket2))\n",
    "'''\n",
    "Finally we add the directory record lists to a directory object as well as setting our globabl depth to be 1\n",
    "since this is a binary depth (2 directory_records)\n",
    "'''\n",
    "directory = Directory(global_depth = 1, directory_records = directory_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insertion Algorithm\n",
    "Here we are using this to insert variables, it is important to know the steps:\n",
    "1. Analyse data - Data elements may exist in many forms e.g int, sting float. So we look at the binary\n",
    "2. Initialise the bucket depth and the global depth of the directories\n",
    "3. Convert into a binary format\n",
    "4. Consider the “Global Depth” number of the least significant bits (LSBs) (bits at the end) of data\n",
    "5. Map the data according to the ID of a directory\n",
    "6. Check the following conditions if a bucket overflows (Exceeds size limit)\n",
    "- Global depth == Bucket depth (local depth) split the bucket into two and increment the global depth and the bucket's depth. Re-hash the elements that were present in the split bucket\n",
    "- Global depth > Bucket depth (local depth): Split the bucket into two and increment the bucket depth only. Re-hash the elements that were present in the split bucket\n",
    "7. Repeat the process for each element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert(index):\n",
    "    \n",
    "    #set the directoy and the bucket number (global since it calls from outside the function)\n",
    "    global directory\n",
    "    global bucket_number\n",
    "    '''\n",
    "    STEP 3\n",
    "    '''\n",
    "    #Take the first variable in the index and hash it\n",
    "    t_id = index[0]\n",
    "    hash_key = hash_funtion(int(t_id))\n",
    "    \n",
    "    '''\n",
    "    STEP 4\n",
    "    '''\n",
    "    #This finds what hash directory matches the inserted value\n",
    "    hash_prefix = int(hash_key[-directory.global_depth[0]:], 2)\n",
    "\n",
    "    '''\n",
    "    STEP 5\n",
    "    '''\n",
    "    #This matches the directory to the bucket based on the hash value\n",
    "    bucket = directory.directory_records[hash_prefix].value\n",
    "    # Add the value to the bucket\n",
    "    bucket.index.append(index)\n",
    "    # Since we put an item inside the bucket we lose a space within it\n",
    "    bucket.empty_spaces = int(bucket.empty_spaces)-1\n",
    "\n",
    "    '''\n",
    "    STEP 6\n",
    "    '''\n",
    "    # This only occurs when there is an overflow\n",
    "    if(bucket.empty_spaces < 0):\n",
    "        \n",
    "        # Create a way to store the bucket index\n",
    "        tempopary_memory = bucket.index   \n",
    "        # Reset the empty spaces \n",
    "        bucket.empty_spaces = bucket_capacity\n",
    "        # Clear its sotred values\n",
    "        bucket.index = []\n",
    "\n",
    "        '''\n",
    "        STEP 6A \n",
    "        occurs if the depth is larger than bucket depth, meaning we only need to split the bucket\n",
    "        '''\n",
    "        if (directory.global_depth[0] > bucket.local_depth):\n",
    "\n",
    "            # NUMBER OF LINKED BUCKETS\n",
    "            number_of_links = 2*(directory.global_depth[0] - bucket.local_depth)\n",
    "            # Increase the depth of the bucket by 1\n",
    "            bucket.local_depth = bucket.local_depth + 1\n",
    "            number_of_modify_links = number_of_links/2 \n",
    "\n",
    "            #Create new bucket\n",
    "            new_bucket = Bucket(local_depth = bucket.local_depth, index=[], empty_spaces = bucket_capacity, id = bucket_number)\n",
    "\n",
    "            #Go through each record \n",
    "            for directory_record in directory.directory_records:\n",
    "                #Check for the bucket\n",
    "                if(directory_record.value == bucket):\n",
    "                    #Each number of links to buckets is then lowered by 1 untill a the new bucket is found\n",
    "                    if(number_of_modify_links != 0):\n",
    "                        number_of_modify_links = number_of_modify_links - 1\n",
    "                    #When it is found link the new bucket and increase the amount of buckets\n",
    "                    else:\n",
    "                        directory_record.value = new_bucket\n",
    "                        bucket_number = bucket_number + 1\n",
    "\n",
    "            # Insert the temprorary bucket back into the list\n",
    "            for i in range(len(tempopary_memory)):\n",
    "                insert(tempopary_memory[i])\n",
    "                \n",
    "            '''\n",
    "            STEP 6B \n",
    "            Occurs when the bucket is the same size as the depth since depth needs to be increased with bucket split\n",
    "            '''\n",
    "        elif (directory.global_depth[0] == bucket.local_depth):\n",
    "            # Create a new length for the directory that is double (since we work in binary)\n",
    "            new_directory_len = 2 * len(directory.directory_records)\n",
    "            # Set a new cleared records list\n",
    "            new_directory_records = []\n",
    "\n",
    "            # Iterate through the length of new directory\n",
    "            for directory_record_number in range(new_directory_len):\n",
    "                \n",
    "                new_directory_records.append(DirectoryRecord(hash_prefix=directory_record_number,bucket=Bucket(local_depth=1,index=[],empty_spaces=bucket_capacity,id=bucket_number)))\n",
    "                # Increase the number of buckets for ID\n",
    "                bucket_number = bucket_number + 1\n",
    "            \n",
    "            # Create the new directory\n",
    "            new_directory = Directory(global_depth=directory.global_depth[0]+1,directory_records=new_directory_records)\n",
    "\n",
    "            # REHASING\n",
    "\n",
    "            #iterate through records\n",
    "            for directory_record in directory.directory_records:\n",
    "                #Adjust the hashes for previous entries \n",
    "                haskey1 = '0'+hash_funtion(directory_record.hash_prefix)\n",
    "                haskey2 = '1'+hash_funtion(directory_record.hash_prefix)\n",
    "                #Crete new hash indexes for them\n",
    "                new_index1 = int(haskey1[-directory.global_depth[0]:],2)\n",
    "                new_index2 = int(haskey2[-directory.global_depth[0]:],2)\n",
    "                #Add the doubled directroeis to the new dorectory\n",
    "                new_directory.directory_records[new_index1].value = directory_record.value\n",
    "                new_directory.directory_records[new_index2].value = directory_record.value\n",
    "            #Set the directy as the new one we had created\n",
    "            directory= new_directory\n",
    "\n",
    "            # We now re-add each value form temporary memory in again with the adjusted hash index. Recursivly\n",
    "            for i in range(len(tempopary_memory)):\n",
    "\n",
    "                insert(tempopary_memory[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A - Simple use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash Prefix:  0000000000000000\n",
      "Bucket ID:  1\n",
      "Local Depth:  1\n",
      "Stored Values:  [[0, 100, 'David'], [4, 102, 'David2']]\n",
      "Hash Prefix:  0000000000000001\n",
      "Bucket ID:  2\n",
      "Local Depth:  1\n",
      "Stored Values:  [[1, 101, 'David2']]\n",
      "Hash Prefix:  0000000000000010\n",
      "Bucket ID:  5\n",
      "Local Depth:  1\n",
      "Stored Values:  [[2, 102, 'David2']]\n",
      "Hash Prefix:  0000000000000011\n",
      "Bucket ID:  6\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n"
     ]
    }
   ],
   "source": [
    "t_id = 0\n",
    "t_amount = 100\n",
    "u_name = 'David'\n",
    "\n",
    "insert([t_id, t_amount, u_name])\n",
    "t_id = 1\n",
    "t_amount = 101\n",
    "u_name = 'David2'\n",
    "insert([t_id, t_amount, u_name])\n",
    "\n",
    "t_id = 2\n",
    "t_amount = 102\n",
    "u_name = 'David2'\n",
    "insert([t_id, t_amount, u_name])\n",
    "\n",
    "t_id = 4\n",
    "t_amount = 102\n",
    "u_name = 'David2'\n",
    "insert([t_id, t_amount, u_name])\n",
    "\n",
    "for i in range(len(directory.directory_records)):\n",
    "    print(\"Hash Prefix: \",hash_funtion(directory.directory_records[i].hash_prefix))\n",
    "    print(\"Bucket ID: \",directory.directory_records[i].value.id)\n",
    "    print(\"Local Depth: \",directory.directory_records[i].value.local_depth)\n",
    "    print(\"Stored Values: \",directory.directory_records[i].value.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash Prefix:  0000000000000000\n",
      "Bucket ID:  1\n",
      "Local Depth:  2\n",
      "Stored Values:  [[0, 100, 'David'], [8, 102, 'David2']]\n",
      "Hash Prefix:  0000000000000001\n",
      "Bucket ID:  2\n",
      "Local Depth:  1\n",
      "Stored Values:  [[1, 101, 'David2']]\n",
      "Hash Prefix:  0000000000000010\n",
      "Bucket ID:  5\n",
      "Local Depth:  1\n",
      "Stored Values:  [[2, 102, 'David2']]\n",
      "Hash Prefix:  0000000000000011\n",
      "Bucket ID:  6\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000000100\n",
      "Bucket ID:  11\n",
      "Local Depth:  1\n",
      "Stored Values:  [[4, 102, 'David2']]\n",
      "Hash Prefix:  0000000000000101\n",
      "Bucket ID:  12\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000000110\n",
      "Bucket ID:  13\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000000111\n",
      "Bucket ID:  14\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n"
     ]
    }
   ],
   "source": [
    "t_id = 8\n",
    "t_amount = 102\n",
    "u_name = 'David2'\n",
    "insert([t_id, t_amount, u_name])\n",
    "\n",
    "for i in range(len(directory.directory_records)):\n",
    "    print(\"Hash Prefix: \",hash_funtion(directory.directory_records[i].hash_prefix))\n",
    "    print(\"Bucket ID: \",directory.directory_records[i].value.id)\n",
    "    print(\"Local Depth: \",directory.directory_records[i].value.local_depth)\n",
    "    print(\"Stored Values: \",directory.directory_records[i].value.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A - Import Random And CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket1 = Bucket(local_depth = 1, empty_spaces = bucket_capacity, index = [], id = 1)\n",
    "bucket2 = Bucket(local_depth = 1, empty_spaces = bucket_capacity, index = [], id = 2)\n",
    "directory_records = list()  \n",
    "directory_records.append(DirectoryRecord(hash_prefix = 0, bucket = bucket1))\n",
    "directory_records.append(DirectoryRecord(hash_prefix = 1, bucket = bucket2))\n",
    "directory = Directory(global_depth = 1, directory_records = directory_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 24, 12, 21, 26, 6, 10, 3, 7, 11, 9, 4, 27]\n"
     ]
    }
   ],
   "source": [
    "#13 random numbers like the task before (part 1)\n",
    "sampleNumbers = random.sample(range(1, 30), 13)\n",
    "print(sampleNumbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"randomNumber.csv\", sampleNumbers, delimiter=\",\", fmt='%s', header='Numbers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['17'], ['24'], ['12'], ['21'], ['26'], ['6'], ['10'], ['3'], ['7'], ['11'], ['9'], ['4'], ['27']]\n"
     ]
    }
   ],
   "source": [
    "file = open(\"randomNumber.csv\", \"r\")\n",
    "importData = list(csv.reader(file, delimiter=\",\"))\n",
    "file.close()\n",
    "importData.pop(0)\n",
    "print(importData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(importData)):\n",
    "    insert(importData[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Depth:  (4,)\n",
      "Count of directory records:  16\n",
      "Hash Prefix:  0000000000000000\n",
      "Bucket ID:  1\n",
      "Local Depth:  1\n",
      "Stored Values:  [['24'], ['12']]\n",
      "Hash Prefix:  0000000000000001\n",
      "Bucket ID:  2\n",
      "Local Depth:  2\n",
      "Stored Values:  [['17'], ['9']]\n",
      "Hash Prefix:  0000000000000010\n",
      "Bucket ID:  17\n",
      "Local Depth:  2\n",
      "Stored Values:  [['26'], ['10']]\n",
      "Hash Prefix:  0000000000000011\n",
      "Bucket ID:  18\n",
      "Local Depth:  3\n",
      "Stored Values:  [['3']]\n",
      "Hash Prefix:  0000000000000100\n",
      "Bucket ID:  23\n",
      "Local Depth:  1\n",
      "Stored Values:  [['4']]\n",
      "Hash Prefix:  0000000000000101\n",
      "Bucket ID:  24\n",
      "Local Depth:  1\n",
      "Stored Values:  [['21']]\n",
      "Hash Prefix:  0000000000000110\n",
      "Bucket ID:  25\n",
      "Local Depth:  1\n",
      "Stored Values:  [['6']]\n",
      "Hash Prefix:  0000000000000111\n",
      "Bucket ID:  26\n",
      "Local Depth:  1\n",
      "Stored Values:  [['7']]\n",
      "Hash Prefix:  0000000000001000\n",
      "Bucket ID:  35\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001001\n",
      "Bucket ID:  36\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001010\n",
      "Bucket ID:  37\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001011\n",
      "Bucket ID:  38\n",
      "Local Depth:  1\n",
      "Stored Values:  [['11'], ['27']]\n",
      "Hash Prefix:  0000000000001100\n",
      "Bucket ID:  39\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001101\n",
      "Bucket ID:  40\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001110\n",
      "Bucket ID:  41\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001111\n",
      "Bucket ID:  42\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n"
     ]
    }
   ],
   "source": [
    "print(\"Global Depth: \", directory.global_depth)\n",
    "print(\"Count of directory records: \", len(directory.directory_records))\n",
    "for i in range(len(directory.directory_records)):\n",
    "    print(\"Hash Prefix: \",hash_funtion(directory.directory_records[i].hash_prefix))\n",
    "    print(\"Bucket ID: \",directory.directory_records[i].value.id)\n",
    "    print(\"Local Depth: \",directory.directory_records[i].value.local_depth)\n",
    "    print(\"Stored Values: \",directory.directory_records[i].value.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B - Modification - Simple refactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDirectory(index):\n",
    "    print(\"Global Depth: \", index.global_depth)\n",
    "    print(\"Count of directory records: \", len(index.directory_records))\n",
    "    for i in range(len(index.directory_records)):\n",
    "        print(\"Hash Prefix: \",hash_funtion(index.directory_records[i].hash_prefix))\n",
    "        print(\"Bucket ID: \",index.directory_records[i].value.id)\n",
    "        print(\"Local Depth: \",index.directory_records[i].value.local_depth)\n",
    "        print(\"Stored Values: \",index.directory_records[i].value.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Depth:  (4,)\n",
      "Count of directory records:  16\n",
      "Hash Prefix:  0000000000000000\n",
      "Bucket ID:  1\n",
      "Local Depth:  1\n",
      "Stored Values:  [['24'], ['12']]\n",
      "Hash Prefix:  0000000000000001\n",
      "Bucket ID:  2\n",
      "Local Depth:  2\n",
      "Stored Values:  [['17'], ['9']]\n",
      "Hash Prefix:  0000000000000010\n",
      "Bucket ID:  17\n",
      "Local Depth:  2\n",
      "Stored Values:  [['26'], ['10']]\n",
      "Hash Prefix:  0000000000000011\n",
      "Bucket ID:  18\n",
      "Local Depth:  3\n",
      "Stored Values:  [['3']]\n",
      "Hash Prefix:  0000000000000100\n",
      "Bucket ID:  23\n",
      "Local Depth:  1\n",
      "Stored Values:  [['4']]\n",
      "Hash Prefix:  0000000000000101\n",
      "Bucket ID:  24\n",
      "Local Depth:  1\n",
      "Stored Values:  [['21']]\n",
      "Hash Prefix:  0000000000000110\n",
      "Bucket ID:  25\n",
      "Local Depth:  1\n",
      "Stored Values:  [['6']]\n",
      "Hash Prefix:  0000000000000111\n",
      "Bucket ID:  26\n",
      "Local Depth:  1\n",
      "Stored Values:  [['7']]\n",
      "Hash Prefix:  0000000000001000\n",
      "Bucket ID:  35\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001001\n",
      "Bucket ID:  36\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001010\n",
      "Bucket ID:  37\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001011\n",
      "Bucket ID:  38\n",
      "Local Depth:  1\n",
      "Stored Values:  [['11'], ['27']]\n",
      "Hash Prefix:  0000000000001100\n",
      "Bucket ID:  39\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001101\n",
      "Bucket ID:  40\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001110\n",
      "Bucket ID:  41\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n",
      "Hash Prefix:  0000000000001111\n",
      "Bucket ID:  42\n",
      "Local Depth:  1\n",
      "Stored Values:  []\n"
     ]
    }
   ],
   "source": [
    "printDirectory(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 External Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our random sample data: \n",
      "[139, 199, 4, 151, 182, 45, 85, 114, 171, 132, 60, 63, 74, 111, 16, 69, 61, 148, 180, 157, 190, 191, 145, 143, 130, 5, 138, 7, 91, 127, 73, 82, 31, 100, 152, 55, 137, 165, 35, 36, 84, 38, 97, 77, 150, 172, 173, 175, 56, 195, 25, 93, 188, 81, 178, 12, 95, 146, 34, 62, 94, 142, 42, 181, 18, 163, 28, 44, 101, 153, 185, 194, 197, 134, 78, 1, 15, 177, 170, 21, 167, 30, 65, 122, 176, 119, 70, 49, 109, 13, 29, 103, 68, 8, 162, 106, 6, 193, 133, 115]\n",
      "Disk data being split into pages = \n",
      "[[115, 133, 193, 6], [106, 162, 8, 68], [103, 29, 13, 109], [49, 70, 119, 176], [122, 65, 30, 167], [21, 170, 177, 15], [1, 78, 134, 197], [194, 185, 153, 101], [44, 28, 163, 18], [181, 42, 142, 94], [62, 34, 146, 95], [12, 178, 81, 188], [93, 25, 195, 56], [175, 173, 172, 150], [77, 97, 38, 84], [36, 35, 165, 137], [55, 152, 100, 31], [82, 73, 127, 91], [7, 138, 5, 130], [143, 145, 191, 190], [157, 180, 148, 61], [69, 16, 111, 74], [63, 60, 132, 171], [114, 85, 45, 182], [151, 4, 199, 139]]\n",
      "Sort each page: \n",
      "[[6, 115, 133, 193], [8, 68, 106, 162], [13, 29, 103, 109], [49, 70, 119, 176], [30, 65, 122, 167], [15, 21, 170, 177], [1, 78, 134, 197], [101, 153, 185, 194], [18, 28, 44, 163], [42, 94, 142, 181], [34, 62, 95, 146], [12, 81, 178, 188], [25, 56, 93, 195], [150, 172, 173, 175], [38, 77, 84, 97], [35, 36, 137, 165], [31, 55, 100, 152], [73, 82, 91, 127], [5, 7, 130, 138], [143, 145, 190, 191], [61, 148, 157, 180], [16, 69, 74, 111], [60, 63, 132, 171], [45, 85, 114, 182], [4, 139, 151, 199]]\n",
      "Now the pages are ordered based on first val: \n",
      "[[1, 78, 134, 197], [4, 139, 151, 199], [5, 7, 130, 138], [6, 115, 133, 193], [8, 68, 106, 162], [12, 81, 178, 188], [13, 29, 103, 109], [15, 21, 170, 177], [16, 69, 74, 111], [18, 28, 44, 163], [25, 56, 93, 195], [30, 65, 122, 167], [31, 55, 100, 152], [34, 62, 95, 146], [35, 36, 137, 165], [38, 77, 84, 97], [42, 94, 142, 181], [45, 85, 114, 182], [49, 70, 119, 176], [60, 63, 132, 171], [61, 148, 157, 180], [73, 82, 91, 127], [101, 153, 185, 194], [143, 145, 190, 191], [150, 172, 173, 175]]\n",
      "Final sorted data on 'disk': \n",
      "[1, 4, 5, 6, 7, 8, 12, 13, 15, 16, 18, 21, 25, 28, 29, 30, 31, 34, 35, 36, 38, 42, 44, 45, 49, 55, 56, 60, 61, 62, 63, 65, 68, 69, 70, 73, 74, 77, 78, 81, 82, 84, 85, 91, 93, 94, 95, 97, 100, 101, 103, 106, 109, 111, 114, 115, 119, 122, 127, 130, 132, 133, 134, 137, 138, 139, 142, 143, 145, 146, 148, 150, 151, 152, 153, 157, 162, 163, 165, 167, 170, 171, 172, 173, 175, 176, 177, 178, 180, 181, 182, 185, 188, 190, 191, 193, 194, 195, 197, 199]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the data we are working with:\n",
    "# How many samples of data\n",
    "sampleCount = 100\n",
    "# Out size of N, 4 as per example\n",
    "pageSize = 4\n",
    "# Calculate how many pages of data that we will need\n",
    "pageCount = math.ceil(sampleCount / pageSize)\n",
    "# This is out simulated storage of disk, as a list\n",
    "diskList = []\n",
    "# This is our sorting heap / buffer\n",
    "sorting = []\n",
    "# \n",
    "diskOut = []\n",
    "\n",
    "# Generate our samples\n",
    "samples = random.sample(range(1, 200), sampleCount)\n",
    "print(\"Our random sample data: \\n{}\".format(samples))\n",
    "\n",
    "#run through and split the data into pages\n",
    "for i in range(pageCount):\n",
    "    if not len(samples):\n",
    "        break\n",
    "    samplePage = []\n",
    "    for j in range(pageSize):\n",
    "        if not len(samples):\n",
    "            break\n",
    "        samplePage.append(samples.pop())\n",
    "    diskList.append(samplePage)\n",
    "\n",
    "print(\"Disk data being split into pages = \\n{}\".format(diskList))\n",
    "\n",
    "# Sort all values within each page\n",
    "for i in range(0, len(diskList)):\n",
    "    initalSort = np.sort(diskList[i])\n",
    "    diskList[i] = initalSort.tolist()\n",
    "\n",
    "print(\"Sort each page: \\n{}\".format(diskList))\n",
    "\n",
    "# Order each page based on the first value\n",
    "diskList = sorted(diskList, key=lambda x : x[0])\n",
    "\n",
    "print(\"Now the pages are ordered based on first val: \\n{}\".format(diskList))\n",
    "\n",
    "# Merge by adding each page to the heap\n",
    "for i in range(len(diskList)):\n",
    "    for j in range(len(diskList[i])):\n",
    "        heap = heapq.heappush(sorting, (int(diskList[i][j]), i))\n",
    "\n",
    "diskList=[]\n",
    "# Loaded into the heap we can take the minimum value\n",
    "for x in range(sampleCount):\n",
    "        # Get the minimum val and append it\n",
    "        root = heapq.heappop(sorting)\n",
    "        diskList.append(int(root[0]))\n",
    "\n",
    "print(\"Final sorted data on 'disk': \\n{}\".format(diskList))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
